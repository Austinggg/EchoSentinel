{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认设备以及cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda,optim \n",
    "print(torch.__version__);\n",
    "print(torch.cuda.is_available())\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3120\\304779165.py:3: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df_train = pd.read_json('../weibo21_all.json', lines=True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3120\\304779165.py:3: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df_train = pd.read_json('../weibo21_all.json', lines=True)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3120\\304779165.py:3: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df_train = pd.read_json('../weibo21_all.json', lines=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>images</th>\n",
       "      <th>content</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'path': 'image/100000_0.jpg'}, {'path': 'ima...</td>\n",
       "      <td>【三星折叠屏原型机曝光：双屏设计/非柔性屏】网友@黎启lee晒出三星的ProjcetV可折叠...</td>\n",
       "      <td>1970-01-18 16:46:49.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'path': 'image/100001_0.jpg'}]</td>\n",
       "      <td>【能电鱼，更能电力找鱼：别以为电鳗只有把鱼电晕一招！】电鳗2米长的身体里有许多专门发电的细胞...</td>\n",
       "      <td>1970-01-17 18:11:57.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'path': 'image/100002_0.jpg'}]</td>\n",
       "      <td>在iPad2发售几小时后，国外玩家就已经对它进行了拆解。想了解如此轻薄的机身如何容纳下这么多...</td>\n",
       "      <td>1970-01-16 01:04:58.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'path': 'image/100003_0.jpg'}]</td>\n",
       "      <td>#乐享时光#现在科学家都这么闲了吗?谢谢你告诉我这么重要的结论啊！（转）</td>\n",
       "      <td>1970-01-17 13:05:04.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'path': 'image/100004_0.jpg'}]</td>\n",
       "      <td>全国首家“无人银行”亮相大堂经理是智能机器人银行里没有忙碌的工作人员和拥挤的排队客户，而是各...</td>\n",
       "      <td>1970-01-18 15:12:18.840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  label  category                                             images  \\\n",
       "0  100000      0         0  [{'path': 'image/100000_0.jpg'}, {'path': 'ima...   \n",
       "1  100001      0         0                   [{'path': 'image/100001_0.jpg'}]   \n",
       "2  100002      0         0                   [{'path': 'image/100002_0.jpg'}]   \n",
       "3  100003      0         0                   [{'path': 'image/100003_0.jpg'}]   \n",
       "4  100004      0         0                   [{'path': 'image/100004_0.jpg'}]   \n",
       "\n",
       "                                             content               timestamp  \n",
       "0  【三星折叠屏原型机曝光：双屏设计/非柔性屏】网友@黎启lee晒出三星的ProjcetV可折叠... 1970-01-18 16:46:49.446  \n",
       "1  【能电鱼，更能电力找鱼：别以为电鳗只有把鱼电晕一招！】电鳗2米长的身体里有许多专门发电的细胞... 1970-01-17 18:11:57.263  \n",
       "2  在iPad2发售几小时后，国外玩家就已经对它进行了拆解。想了解如此轻薄的机身如何容纳下这么多... 1970-01-16 01:04:58.932  \n",
       "3               #乐享时光#现在科学家都这么闲了吗?谢谢你告诉我这么重要的结论啊！（转） 1970-01-17 13:05:04.203  \n",
       "4  全国首家“无人银行”亮相大堂经理是智能机器人银行里没有忙碌的工作人员和拥挤的排队客户，而是各... 1970-01-18 15:12:18.840  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "df_train = pd.read_json('../weibo21_all.json', lines=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取标签以及输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X = df_train['content']\n",
    "Y = df_train['label'] \n",
    "print(df_train['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载bert作为分词模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,BertModel\n",
    "# 指定本地路径加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"E:/mod/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 使用BERT分词器进行编码\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1740, 1840,  ...,    0,    0,    0],\n",
      "        [ 101, 1001,  100,  ...,  100, 1740,  102],\n",
      "        [ 101, 1001,  100,  ..., 1825, 1742,  102],\n",
      "        ...,\n",
      "        [ 101,  100, 1811,  ...,    0,    0,    0],\n",
      "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
      "        [ 101, 1523,  100,  ...,  100, 1909,  102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0])}\n"
     ]
    }
   ],
   "source": [
    "# 最大句子长度\n",
    "max_length = 128\n",
    "\n",
    "# 创建 Dataset 实例\n",
    "train_dataset = SentimentDataset(X, Y, tokenizer=tokenizer, max_length=max_length)\n",
    "\n",
    "# 设置训练集和验证集的比例，比如 80% 训练集，20% 验证集\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "# 拆分数据集\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# 查看一个 batch 的数据\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break  # 只查看第一个 batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.7):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('E:/mod/bert-base-uncased')  # 加载预训练的 BERT 模型\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout 层，防止过拟合\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)  # 分类层\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 使用 BERT 获取隐藏状态（默认返回最后一层的所有隐藏状态）\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output.pooler_output  # 获取 [CLS] token 的表示\n",
    "        dropout_output = self.dropout(pooled_output)  # 经过 Dropout\n",
    "        logits = self.linear(dropout_output)  # 线性层输出分类结果\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "num_classes = len(df_train['label'].unique())  # 类别数量\n",
    "model = BERTClassifier(num_classes)  # 使用自定义的 BERT 分类模型\n",
    "model.to(device)  # 将模型移动到设备上\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 适用于分类任务\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5,weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估函数\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "# 定义评估函数\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():  # 不需要计算梯度\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # 记录模型的预测值（取概率最大的类别）\n",
    "            preds = F.softmax(outputs, dim=1).argmax(dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6506\n",
      "Epoch 1/100, Validation Accuracy: 0.7119\n",
      "Epoch 2/100, Loss: 0.5770\n",
      "Epoch 2/100, Validation Accuracy: 0.7377\n",
      "Epoch 3/100, Loss: 0.4985\n",
      "Epoch 3/100, Validation Accuracy: 0.7645\n",
      "Epoch 4/100, Loss: 0.4431\n",
      "Epoch 4/100, Validation Accuracy: 0.7859\n",
      "Epoch 5/100, Loss: 0.3741\n",
      "Epoch 5/100, Validation Accuracy: 0.7908\n",
      "Epoch 6/100, Loss: 0.3205\n",
      "Epoch 6/100, Validation Accuracy: 0.7963\n",
      "Epoch 7/100, Loss: 0.2807\n",
      "Epoch 7/100, Validation Accuracy: 0.8248\n",
      "Epoch 8/100, Loss: 0.2430\n",
      "Epoch 8/100, Validation Accuracy: 0.8149\n",
      "Epoch 9/100, Loss: 0.2040\n",
      "Epoch 9/100, Validation Accuracy: 0.8363\n",
      "Epoch 10/100, Loss: 0.1787\n",
      "Epoch 10/100, Validation Accuracy: 0.8220\n",
      "Epoch 11/100, Loss: 0.1468\n",
      "Epoch 11/100, Validation Accuracy: 0.8401\n",
      "Epoch 12/100, Loss: 0.1252\n",
      "Epoch 12/100, Validation Accuracy: 0.8368\n",
      "Epoch 13/100, Loss: 0.0982\n",
      "Epoch 13/100, Validation Accuracy: 0.8324\n",
      "Epoch 14/100, Loss: 0.0811\n",
      "Epoch 14/100, Validation Accuracy: 0.8445\n",
      "Epoch 15/100, Loss: 0.0776\n",
      "Epoch 15/100, Validation Accuracy: 0.8445\n",
      "Epoch 16/100, Loss: 0.0603\n",
      "Epoch 16/100, Validation Accuracy: 0.8368\n",
      "Epoch 17/100, Loss: 0.0433\n",
      "Epoch 17/100, Validation Accuracy: 0.8308\n",
      "Epoch 18/100, Loss: 0.0576\n",
      "Epoch 18/100, Validation Accuracy: 0.8412\n",
      "Epoch 19/100, Loss: 0.0565\n",
      "Epoch 19/100, Validation Accuracy: 0.8242\n",
      "Epoch 20/100, Loss: 0.0388\n",
      "Epoch 20/100, Validation Accuracy: 0.8417\n",
      "Epoch 21/100, Loss: 0.0312\n",
      "Epoch 21/100, Validation Accuracy: 0.8379\n",
      "Epoch 22/100, Loss: 0.0211\n",
      "Epoch 22/100, Validation Accuracy: 0.8461\n",
      "Epoch 23/100, Loss: 0.0335\n",
      "Epoch 23/100, Validation Accuracy: 0.8461\n",
      "Epoch 24/100, Loss: 0.0262\n",
      "Epoch 24/100, Validation Accuracy: 0.8182\n",
      "Epoch 25/100, Loss: 0.0276\n",
      "Epoch 25/100, Validation Accuracy: 0.8423\n",
      "Epoch 26/100, Loss: 0.0294\n",
      "Epoch 26/100, Validation Accuracy: 0.8226\n",
      "Epoch 27/100, Loss: 0.0198\n",
      "Epoch 27/100, Validation Accuracy: 0.8467\n",
      "Epoch 28/100, Loss: 0.0183\n",
      "Epoch 28/100, Validation Accuracy: 0.8341\n",
      "Epoch 29/100, Loss: 0.0297\n",
      "Epoch 29/100, Validation Accuracy: 0.8412\n",
      "Epoch 30/100, Loss: 0.0219\n",
      "Epoch 30/100, Validation Accuracy: 0.8450\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化存储损失和准确率的列表\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)  # 记录训练损失\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # 每个 epoch 后进行评估\n",
    "    val_accuracy = evaluate(model, val_loader)\n",
    "    val_accuracies.append(val_accuracy)  # 记录验证准确率\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "# 绘制损失和验证准确率图\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 绘制损失图\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', color='b', label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(1, len(train_losses) + 1))  # 设置 x 轴刻度\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# 绘制验证准确率图\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, marker='o', color='g', label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(range(1, len(val_accuracies) + 1))  # 设置 x 轴刻度\n",
    "plt.ylim(0, 1)  # 限定 y 轴范围\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # 自动调整子图参数\n",
    "plt.show()  # 显示图像\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
