{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "from models.dnf import DNF, DeltaDelayedExponentialDecayScheduler\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class WeiboDataset(Dataset):\n",
    "    def __init__(self, mongo_uri=None, db_name=None, collection_name=None, folder_path=None, selected_features=None):\n",
    "        # 设置默认特征（如果未指定则使用P1-P8）\n",
    "        self.selected_features = selected_features if selected_features else [f\"P{i}\" for i in range(1, 9)]\n",
    "        records = []\n",
    "        \n",
    "        # 从MongoDB读取数据\n",
    "        if mongo_uri and db_name and collection_name:\n",
    "            try:\n",
    "                client = MongoClient(mongo_uri)\n",
    "                db = client[db_name]\n",
    "                collection = db[collection_name]\n",
    "                \n",
    "                # 查询文档\n",
    "                cursor = collection.find({})\n",
    "                for doc in cursor:\n",
    "                    records.append(doc)\n",
    "                \n",
    "                print(f\"从MongoDB加载了 {len(records)} 条记录\")\n",
    "                client.close()\n",
    "            except Exception as e:\n",
    "                print(f\"连接MongoDB出错: {str(e)}\")\n",
    "                return\n",
    "        # 从本地文件加载数据（保留原功能作为备选）\n",
    "        elif folder_path:\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.json'):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                        records.append(data)\n",
    "        else:\n",
    "            raise ValueError(\"必须提供MongoDB连接信息或本地数据文件夹路径\")\n",
    "            \n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for item in records:\n",
    "            try:\n",
    "                # 显式按顺序提取P1-P8并处理空值\n",
    "                p_values = []\n",
    "                for key in self.selected_features:  # 改为遍历选中特征\n",
    "                    val = item.get('general', {}).get(key, 0.0)  # 获取值，若缺失则默认为0.0\n",
    "                    # 处理空字符串\n",
    "                    if isinstance(val, str):\n",
    "                        if val.strip() == \"\":\n",
    "                            val = 0.0  # 空字符串替换为0.0\n",
    "                        else:\n",
    "                            val = float(val)  # 尝试转换非空字符串\n",
    "                    p_values.append(float(val))\n",
    "                \n",
    "                self.features.append(p_values)\n",
    "                \n",
    "                # 标签处理（兼容多种格式）\n",
    "                label_val = item.get('label', 0)\n",
    "                label = 1 if str(label_val).strip() in (\"1\", \"true\", \"True\") else 0\n",
    "                self.labels.append(label)\n",
    "                \n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(f\"数据 {item.get('_id', '未知')} 异常: {str(e)}\")\n",
    "                continue  # 跳过无效样本\n",
    "                \n",
    "        self.num_features = len(self.selected_features)\n",
    "        # 新增诊断信息\n",
    "        print(f\"\\n数据集诊断信息:\")\n",
    "        print(f\"总样本数: {len(self.labels)}\")\n",
    "        if len(self.labels) > 0:\n",
    "            print(f\"正样本比例: {sum(self.labels)/len(self.labels):.2%}\")\n",
    "            print(f\"特征维度: {len(self.features[0]) if self.features else 0}\")\n",
    "            print(f\"示例特征: {self.features[0] if self.features else []}\")\n",
    "            print(f\"对应标签: {self.labels[0] if self.labels else []}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        one_hot = torch.zeros(2)\n",
    "        one_hot[label] = 1.0\n",
    "        return torch.FloatTensor(self.features[idx]), one_hot# MongoDB连接信息\n",
    "mongo_uri = \"mongodb://root:CV3d!GXxZp4aApx@1.95.190.99:27017/admin\"\n",
    "db_name = \"TRAING_DATA\"  # 或者您实际使用的数据库名\n",
    "collection_name = \"WEIBO_21\"  # 实际存储数据的集合名\n",
    "\n",
    "# 特征选择\n",
    "selected_features = [\"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\", \"P8\"]  # 可自由组合\n",
    "\n",
    "# 使用MongoDB加载数据\n",
    "dataset = WeiboDataset(\n",
    "    mongo_uri=mongo_uri,\n",
    "    db_name=db_name,\n",
    "    collection_name=collection_name,\n",
    "    selected_features=selected_features\n",
    ")\n",
    "\n",
    "# 输出加载结果\n",
    "print(f\"成功加载 {len(dataset)} 个样本\")\n",
    "if len(dataset) > 0:\n",
    "    print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNFClassifier(nn.Module):\n",
    "    def __init__(self, num_preds, num_conjuncts, n_out, delta=0.01, weight_init_type=\"normal\"):\n",
    "        super(DNFClassifier, self).__init__()\n",
    "        self.dnf = DNF(num_preds, num_conjuncts, n_out, delta, weight_init_type=weight_init_type)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.dnf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数配置\n",
    "num_preds = dataset.num_features       # P1-P8特征维度\n",
    "num_conjuncts = 10  # 合取项数量（可调整）\n",
    "n_out = 2         # 二分类输出\n",
    "\n",
    "model = DNFClassifier(num_preds, num_conjuncts, n_out)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Delta调度器（关键参数需根据实验调整）\n",
    "delta_scheduler = DeltaDelayedExponentialDecayScheduler(\n",
    "    initial_delta=0.01,\n",
    "    delta_decay_delay=100,\n",
    "    delta_decay_steps=50,\n",
    "    delta_decay_rate=0.1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 数据加载及划分训练集和验证集\n",
    "train_size = int(0.8 * len(dataset))      # 80%作为训练集\n",
    "val_size = len(dataset) - train_size      # 剩余作为验证集\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# ------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 0.6802, Validation Loss: 0.6679, Accuracy: 0.5274\n",
      "Epoch 2/50, Training Loss: 0.6439, Validation Loss: 0.6225, Accuracy: 0.6946\n",
      "Epoch 3/50, Training Loss: 0.6024, Validation Loss: 0.5957, Accuracy: 0.7123\n",
      "Epoch 4/50, Training Loss: 0.5854, Validation Loss: 0.5891, Accuracy: 0.7135\n",
      "Epoch 5/50, Training Loss: 0.5807, Validation Loss: 0.5868, Accuracy: 0.7095\n",
      "Epoch 6/50, Training Loss: 0.5791, Validation Loss: 0.5843, Accuracy: 0.7100\n",
      "Epoch 7/50, Training Loss: 0.5783, Validation Loss: 0.5835, Accuracy: 0.7112\n",
      "Epoch 8/50, Training Loss: 0.5772, Validation Loss: 0.5835, Accuracy: 0.7118\n",
      "Epoch 9/50, Training Loss: 0.5767, Validation Loss: 0.5823, Accuracy: 0.7100\n",
      "Epoch 10/50, Training Loss: 0.5765, Validation Loss: 0.5820, Accuracy: 0.7123\n",
      "Epoch 11/50, Training Loss: 0.5759, Validation Loss: 0.5817, Accuracy: 0.7106\n",
      "Epoch 12/50, Training Loss: 0.5761, Validation Loss: 0.5810, Accuracy: 0.7123\n",
      "Epoch 13/50, Training Loss: 0.5757, Validation Loss: 0.5814, Accuracy: 0.7106\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 14/50, Training Loss: 0.5756, Validation Loss: 0.5812, Accuracy: 0.7118\n",
      "EarlyStopping Trigger Times: 2\n",
      "Epoch 15/50, Training Loss: 0.5756, Validation Loss: 0.5814, Accuracy: 0.7140\n",
      "EarlyStopping Trigger Times: 3\n",
      "Epoch 16/50, Training Loss: 0.5748, Validation Loss: 0.5815, Accuracy: 0.7112\n",
      "EarlyStopping Trigger Times: 4\n",
      "Epoch 17/50, Training Loss: 0.5751, Validation Loss: 0.5804, Accuracy: 0.7140\n",
      "Epoch 18/50, Training Loss: 0.5744, Validation Loss: 0.5806, Accuracy: 0.7140\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 19/50, Training Loss: 0.5748, Validation Loss: 0.5800, Accuracy: 0.7129\n",
      "Epoch 20/50, Training Loss: 0.5743, Validation Loss: 0.5812, Accuracy: 0.7135\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 21/50, Training Loss: 0.5745, Validation Loss: 0.5800, Accuracy: 0.7140\n",
      "Epoch 22/50, Training Loss: 0.5745, Validation Loss: 0.5799, Accuracy: 0.7140\n",
      "Epoch 23/50, Training Loss: 0.5742, Validation Loss: 0.5798, Accuracy: 0.7135\n",
      "Epoch 24/50, Training Loss: 0.5744, Validation Loss: 0.5802, Accuracy: 0.7140\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 25/50, Training Loss: 0.5738, Validation Loss: 0.5798, Accuracy: 0.7135\n",
      "EarlyStopping Trigger Times: 2\n",
      "Epoch 26/50, Training Loss: 0.5740, Validation Loss: 0.5802, Accuracy: 0.7118\n",
      "EarlyStopping Trigger Times: 3\n",
      "Epoch 27/50, Training Loss: 0.5739, Validation Loss: 0.5797, Accuracy: 0.7140\n",
      "Epoch 28/50, Training Loss: 0.5736, Validation Loss: 0.5794, Accuracy: 0.7135\n",
      "Epoch 29/50, Training Loss: 0.5738, Validation Loss: 0.5792, Accuracy: 0.7140\n",
      "Epoch 30/50, Training Loss: 0.5730, Validation Loss: 0.5802, Accuracy: 0.7152\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 31/50, Training Loss: 0.5731, Validation Loss: 0.5797, Accuracy: 0.7118\n",
      "EarlyStopping Trigger Times: 2\n",
      "Epoch 32/50, Training Loss: 0.5737, Validation Loss: 0.5787, Accuracy: 0.7146\n",
      "Epoch 33/50, Training Loss: 0.5730, Validation Loss: 0.5795, Accuracy: 0.7152\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 34/50, Training Loss: 0.5726, Validation Loss: 0.5783, Accuracy: 0.7158\n",
      "Epoch 35/50, Training Loss: 0.5729, Validation Loss: 0.5784, Accuracy: 0.7152\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 36/50, Training Loss: 0.5726, Validation Loss: 0.5779, Accuracy: 0.7158\n",
      "Epoch 37/50, Training Loss: 0.5724, Validation Loss: 0.5775, Accuracy: 0.7152\n",
      "Epoch 38/50, Training Loss: 0.5719, Validation Loss: 0.5773, Accuracy: 0.7146\n",
      "Epoch 39/50, Training Loss: 0.5715, Validation Loss: 0.5769, Accuracy: 0.7158\n",
      "Epoch 40/50, Training Loss: 0.5710, Validation Loss: 0.5769, Accuracy: 0.7140\n",
      "Epoch 41/50, Training Loss: 0.5712, Validation Loss: 0.5765, Accuracy: 0.7129\n",
      "Epoch 42/50, Training Loss: 0.5702, Validation Loss: 0.5756, Accuracy: 0.7175\n",
      "Epoch 43/50, Training Loss: 0.5699, Validation Loss: 0.5757, Accuracy: 0.7129\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 44/50, Training Loss: 0.5695, Validation Loss: 0.5748, Accuracy: 0.7186\n",
      "Epoch 45/50, Training Loss: 0.5690, Validation Loss: 0.5743, Accuracy: 0.7175\n",
      "Epoch 46/50, Training Loss: 0.5684, Validation Loss: 0.5739, Accuracy: 0.7169\n",
      "Epoch 47/50, Training Loss: 0.5682, Validation Loss: 0.5736, Accuracy: 0.7186\n",
      "Epoch 48/50, Training Loss: 0.5678, Validation Loss: 0.5733, Accuracy: 0.7152\n",
      "Epoch 49/50, Training Loss: 0.5677, Validation Loss: 0.5732, Accuracy: 0.7140\n",
      "Epoch 50/50, Training Loss: 0.5672, Validation Loss: 0.5730, Accuracy: 0.7135\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练参数\n",
    "num_epochs = 50\n",
    "patience = 5  # 早停容忍的 epoch 数量\n",
    "best_val_loss = float(\"inf\")\n",
    "trigger_times = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --------------------- 训练阶段 ---------------------\n",
    "    model.train()\n",
    "    running_train_loss = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 更新 delta 值（假设 delta_scheduler 及 model.dnf 已定义）\n",
    "        current_step = epoch * len(train_loader) + step\n",
    "        delta_scheduler.step(model.dnf, current_step)\n",
    "        \n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    \n",
    "    # --------------------- 验证阶段 ---------------------\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    corrects = 0  # 累计正确预测数量\n",
    "    # 修改验证阶段代码部分\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "            \n",
    "            # 二分类预测（阈值0.5）\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            true_labels = labels.argmax(dim=1)\n",
    "            corrects += (predicted == true_labels).sum().item()  # 计算正确预测数\n",
    "            \n",
    "    avg_val_loss = total_loss / total_samples\n",
    "    accuracy = corrects / total_samples\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, \" \n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # --------------------- 早停策略 ---------------------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "        # 可以在这里保存模型最佳权重\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        print(f\"EarlyStopping Trigger Times: {trigger_times}\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# 保存最佳模型\n",
    "torch.save(best_model_state, \"./models/best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7135\n",
      "Validation F1 Score: 0.7161\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.69      0.71       895\n",
      "           1       0.69      0.74      0.72       857\n",
      "\n",
      "    accuracy                           0.71      1752\n",
      "   macro avg       0.71      0.71      0.71      1752\n",
      "weighted avg       0.71      0.71      0.71      1752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 加载保存的最佳模型状态\n",
    "model.load_state_dict(torch.load(\"./models/best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        # 获取预测类别和真实类别的索引，而不是one-hot编码\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        true_labels = labels.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(true_labels)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# 现在可以正常计算评估指标\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')  # 对于类别索引，可以使用binary\n",
    "\n",
    "print(\"Validation Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Validation F1 Score: {:.4f}\".format(f1))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 合取规则 ====\n",
      "conj0 = P2 ∧ ¬P5\n",
      "conj1 = ¬P5\n",
      "conj2 = P2 ∧ ¬P5\n",
      "conj3 = P2 ∧ ¬P5\n",
      "conj4 = P2 ∧ ¬P5\n",
      "conj5 = P2\n",
      "conj6 = P2 ∧ ¬P3 ∧ ¬P4 ∧ ¬P5 ∧ ¬P6\n",
      "conj7 = ¬P2\n",
      "conj8 = P2\n",
      "conj9 = P2\n",
      "\n",
      "==== 最终分类规则 ====\n",
      "Class 0: conj2 ∨ conj4 ∨ conj5 ∨ conj6 ∨ ¬conj7 ∨ conj9\n",
      "Class 1: ¬conj1 ∨ ¬conj2 ∨ ¬conj3 ∨ ¬conj4 ∨ ¬conj5 ∨ ¬conj6 ∨ conj7 ∨ ¬conj9\n"
     ]
    }
   ],
   "source": [
    "# 训练完成后调用\n",
    "rules = model.dnf.get_rules(threshold=0.5)  # 可调整阈值\n",
    "\n",
    "# selected_features\n",
    "print(\"==== 合取规则 ====\")\n",
    "for rule in rules[\"conjuncts\"]:\n",
    "    if \"∅\" not in rule:  # 过滤空规则\n",
    "        print(rule)\n",
    "\n",
    "print(\"\\n==== 最终分类规则 ====\")\n",
    "for class_idx, rule in rules[\"disjuncts\"].items():\n",
    "    print(f\"Class {class_idx}: {rule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explain_rule(rule: str, feature_map: dict) -> str:\n",
    "#     \"\"\"将符号规则转换为自然语言\"\"\"\n",
    "#     explanation = []\n",
    "#     for term in rule.split():\n",
    "#         if \"P\" in term:\n",
    "#             # 提取特征和符号\n",
    "#             sign = \"非\" if \"¬\" in term else \"\"\n",
    "#             p_num = term.split(\"P\")[-1]\n",
    "#             explanation.append(f\"{sign}{feature_map[int(p_num)]}\")\n",
    "#         elif \"conj\" in term:\n",
    "#             explanation.append(f\"({term})\")\n",
    "#     return \" 或 \".join(explanation).replace(\"∧\", \" 且 \").replace(\"∨\", \" 或 \")\n",
    "# feature_map = {\n",
    "#     1: \"信息充分性\", 2: \"信息准确性\", \n",
    "#     3: \"内容完整性\", 4: \"意图正当性\",\n",
    "#     5: \"发布者信誉\", 6: \"情感中立性\",\n",
    "#     7: \"无诱导行为\", 8: \"信息一致性\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 基础特征组合规则\n",
      "- 当 信息准确性 或 非发布者信誉 时触发该规则\n",
      "- 当 非发布者信誉 时触发该规则\n",
      "- 当 信息准确性 或 非发布者信誉 时触发该规则\n",
      "- 当 信息准确性 或 非发布者信誉 时触发该规则\n",
      "- 当 信息准确性 或 非发布者信誉 时触发该规则\n",
      "- 当 信息准确性 时触发该规则\n",
      "- 当 信息准确性 或 非内容完整性 或 非意图正当性 或 非发布者信誉 或 非情感中立性 时触发该规则\n",
      "- 当 非信息准确性 时触发该规则\n",
      "- 当 信息准确性 时触发该规则\n",
      "- 当 信息准确性 时触发该规则\n",
      "\n",
      "## 最终决策逻辑\n",
      "### 虚假信息判定条件\n",
      "满足以下任一条件即判定为虚假信息：\n",
      "  - (conj2)\n",
      "  - (conj4)\n",
      "  - (conj5)\n",
      "  - (conj6)\n",
      "  - (¬conj7)\n",
      "  - (conj9)\n",
      "### 真实信息判定条件\n",
      "满足以下任一条件即判定为真实信息：\n",
      "  - (¬conj1)\n",
      "  - (¬conj2)\n",
      "  - (¬conj3)\n",
      "  - (¬conj4)\n",
      "  - (¬conj5)\n",
      "  - (¬conj6)\n",
      "  - (conj7)\n",
      "  - (¬conj9)\n"
     ]
    }
   ],
   "source": [
    "# def generate_semantic_report(rules, feature_map):\n",
    "#     \"\"\"生成完整语义报告\"\"\"\n",
    "#     report = []\n",
    "#     # 合取规则解释\n",
    "#     report.append(\"## 基础特征组合规则\")\n",
    "#     for conj in rules[\"conjuncts\"]:\n",
    "#         if \"∅\" not in conj:\n",
    "#             _, expr = conj.split(\"=\")\n",
    "#             report.append(f\"- 当 {explain_rule(expr.strip(), feature_map)} 时触发该规则\")\n",
    "    \n",
    "#     # 最终决策规则解释\n",
    "#     report.append(\"\\n## 最终决策逻辑\")\n",
    "#     for cls, rule in rules[\"disjuncts\"].items():\n",
    "#         if \"∅\" not in rule:\n",
    "#             cls_name = \"虚假信息\" if cls == 0 else \"真实信息\"\n",
    "#             report.append(f\"### {cls_name}判定条件\")\n",
    "#             report.append(f\"满足以下任一条件即判定为{cls_name}：\")\n",
    "#             for term in rule.split(\"∨\"):\n",
    "#                 report.append(f\"  - {explain_rule(term.strip(), feature_map)}\")\n",
    "#     return \"\\n\".join(report)\n",
    "# print(generate_semantic_report(rules, feature_map))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
