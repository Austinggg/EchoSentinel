{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据集诊断信息:\n",
      "总样本数: 7283\n",
      "正样本比例: 46.24%\n",
      "特征维度: 8\n",
      "示例特征: [0.7, 0.5, 0.9, 0.8, 0.5, 1.0, 0.8, 1.0]\n",
      "对应标签: 0\n",
      "成功加载 7283 个样本\n",
      "(tensor([0.7000, 0.5000, 0.9000, 0.8000, 0.5000, 1.0000, 0.8000, 1.0000]), tensor([0.]))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os  \n",
    "from models.dnf import DNF, DeltaDelayedExponentialDecayScheduler\n",
    "import numpy as np\n",
    "class WeiboDataset(Dataset):\n",
    "    def __init__(self, folder_path, selected_features=None):  # 新增selected_features参数\n",
    "        # 设置默认特征（如果未指定则使用P1-P8）\n",
    "        self.selected_features = selected_features if selected_features else [f\"P{i}\" for i in range(1, 9)]\n",
    "        records = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                # 修改：添加 encoding=\"utf-8\"\n",
    "                with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    records.append(data)\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for item in records:\n",
    "            try:\n",
    "                # 显式按顺序提取P1-P8并处理空值\n",
    "                p_values = []\n",
    "                for key in self.selected_features:  # 改为遍历选中特征\n",
    "                    val = item['general'].get(key, 0.0)  # 获取值，若缺失则默认为0.0\n",
    "                    # 处理空字符串\n",
    "                    if isinstance(val, str):\n",
    "                        if val.strip() == \"\":\n",
    "                            val = 0.0  # 空字符串替换为0.0\n",
    "                        else:\n",
    "                            val = float(val)  # 尝试转换非空字符串\n",
    "                    p_values.append(float(val))\n",
    "                \n",
    "                self.features.append(p_values)\n",
    "                \n",
    "                # 标签处理（兼容多种格式）\n",
    "                label_val = item.get('label', 0)\n",
    "                label = 1 if str(label_val).strip() in (\"1\", \"true\", \"True\") else 0\n",
    "                self.labels.append(label)\n",
    "                \n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(f\"文件 {item.get('id', '未知')} 数据异常: {str(e)}\")\n",
    "                continue  # 跳过无效样本\n",
    "        self.num_features = len(self.selected_features)\n",
    "        # 新增诊断信息\n",
    "        print(f\"\\n数据集诊断信息:\")\n",
    "        print(f\"总样本数: {len(self.labels)}\")\n",
    "        print(f\"正样本比例: {sum(self.labels)/len(self.labels):.2%}\")\n",
    "        print(f\"特征维度: {len(self.features[0]) if self.features else 0}\")\n",
    "        print(f\"示例特征: {self.features[0] if self.features else []}\")\n",
    "        print(f\"对应标签: {self.labels[0] if self.labels else []}\")\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.features[idx]), torch.FloatTensor([self.labels[idx]])\n",
    "selected_features = [\"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\",\"P8\"]  # 可自由组合\n",
    "dataset = WeiboDataset(\"./data/processed/weibo_21_well\",selected_features=selected_features)  # 加载数据集\n",
    "print(f\"成功加载 {len(dataset)} 个样本\")  # 先确认是否加载到数据\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNFClassifier(nn.Module):\n",
    "    def __init__(self, num_preds, num_conjuncts, n_out, delta=0.01, weight_init_type=\"normal\"):\n",
    "        super(DNFClassifier, self).__init__()\n",
    "        self.dnf = DNF(num_preds, num_conjuncts, n_out, delta, weight_init_type=weight_init_type)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.dnf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数配置\n",
    "num_preds = dataset.num_features       # P1-P8特征维度\n",
    "num_conjuncts = 10  # 合取项数量（可调整）\n",
    "n_out = 1           # 二分类输出\n",
    "\n",
    "model = DNFClassifier(num_preds, num_conjuncts, n_out)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Delta调度器（关键参数需根据实验调整）\n",
    "delta_scheduler = DeltaDelayedExponentialDecayScheduler(\n",
    "    initial_delta=0.01,\n",
    "    delta_decay_delay=100,\n",
    "    delta_decay_steps=50,\n",
    "    delta_decay_rate=0.1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 数据加载及划分训练集和验证集\n",
    "train_size = int(0.8 * len(dataset))      # 80%作为训练集\n",
    "val_size = len(dataset) - train_size      # 剩余作为验证集\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# ------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 0.6824, Validation Loss: 0.6609, Accuracy: 0.5724\n",
      "Epoch 2/50, Training Loss: 0.6487, Validation Loss: 0.6173, Accuracy: 0.6973\n",
      "Epoch 3/50, Training Loss: 0.6043, Validation Loss: 0.5800, Accuracy: 0.7502\n",
      "Epoch 4/50, Training Loss: 0.5724, Validation Loss: 0.5567, Accuracy: 0.7467\n",
      "Epoch 5/50, Training Loss: 0.5586, Validation Loss: 0.5528, Accuracy: 0.7495\n",
      "Epoch 6/50, Training Loss: 0.5546, Validation Loss: 0.5489, Accuracy: 0.7481\n",
      "Epoch 7/50, Training Loss: 0.5511, Validation Loss: 0.5468, Accuracy: 0.7461\n",
      "Epoch 8/50, Training Loss: 0.5498, Validation Loss: 0.5448, Accuracy: 0.7433\n",
      "Epoch 9/50, Training Loss: 0.5495, Validation Loss: 0.5448, Accuracy: 0.7461\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 10/50, Training Loss: 0.5489, Validation Loss: 0.5440, Accuracy: 0.7440\n",
      "Epoch 11/50, Training Loss: 0.5485, Validation Loss: 0.5443, Accuracy: 0.7461\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 12/50, Training Loss: 0.5478, Validation Loss: 0.5438, Accuracy: 0.7447\n",
      "Epoch 13/50, Training Loss: 0.5481, Validation Loss: 0.5433, Accuracy: 0.7426\n",
      "Epoch 14/50, Training Loss: 0.5478, Validation Loss: 0.5437, Accuracy: 0.7440\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 15/50, Training Loss: 0.5476, Validation Loss: 0.5432, Accuracy: 0.7433\n",
      "Epoch 16/50, Training Loss: 0.5472, Validation Loss: 0.5437, Accuracy: 0.7433\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 17/50, Training Loss: 0.5469, Validation Loss: 0.5432, Accuracy: 0.7440\n",
      "EarlyStopping Trigger Times: 2\n",
      "Epoch 18/50, Training Loss: 0.5478, Validation Loss: 0.5431, Accuracy: 0.7440\n",
      "Epoch 19/50, Training Loss: 0.5469, Validation Loss: 0.5424, Accuracy: 0.7426\n",
      "Epoch 20/50, Training Loss: 0.5466, Validation Loss: 0.5454, Accuracy: 0.7474\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 21/50, Training Loss: 0.5468, Validation Loss: 0.5420, Accuracy: 0.7433\n",
      "Epoch 22/50, Training Loss: 0.5472, Validation Loss: 0.5427, Accuracy: 0.7433\n",
      "EarlyStopping Trigger Times: 1\n",
      "Epoch 23/50, Training Loss: 0.5469, Validation Loss: 0.5421, Accuracy: 0.7433\n",
      "EarlyStopping Trigger Times: 2\n",
      "Epoch 24/50, Training Loss: 0.5465, Validation Loss: 0.5437, Accuracy: 0.7474\n",
      "EarlyStopping Trigger Times: 3\n",
      "Epoch 25/50, Training Loss: 0.5468, Validation Loss: 0.5423, Accuracy: 0.7440\n",
      "EarlyStopping Trigger Times: 4\n",
      "Epoch 26/50, Training Loss: 0.5462, Validation Loss: 0.5456, Accuracy: 0.7481\n",
      "EarlyStopping Trigger Times: 5\n",
      "Early stopping!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练参数\n",
    "num_epochs = 50\n",
    "patience = 5  # 早停容忍的 epoch 数量\n",
    "best_val_loss = float(\"inf\")\n",
    "trigger_times = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --------------------- 训练阶段 ---------------------\n",
    "    model.train()\n",
    "    running_train_loss = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 更新 delta 值（假设 delta_scheduler 及 model.dnf 已定义）\n",
    "        current_step = epoch * len(train_loader) + step\n",
    "        delta_scheduler.step(model.dnf, current_step)\n",
    "        \n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    \n",
    "    # --------------------- 验证阶段 ---------------------\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    corrects = 0  # 累计正确预测数量\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "            \n",
    "            # 二分类预测（阈值0.5）\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            corrects += (predicted == labels).sum().item()\n",
    "            \n",
    "    avg_val_loss = total_loss / total_samples\n",
    "    accuracy = corrects / total_samples\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, \" \n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # --------------------- 早停策略 ---------------------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "        # 可以在这里保存模型最佳权重\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        print(f\"EarlyStopping Trigger Times: {trigger_times}\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# 保存最佳模型\n",
    "torch.save(best_model_state, \"./models/best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7481\n",
      "Validation F1 Score: 0.7230\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.75      0.77       811\n",
      "         1.0       0.71      0.74      0.72       646\n",
      "\n",
      "    accuracy                           0.75      1457\n",
      "   macro avg       0.75      0.75      0.75      1457\n",
      "weighted avg       0.75      0.75      0.75      1457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 加载保存的最佳模型状态\n",
    "model.load_state_dict(torch.load(\"./models/best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        # 二分类预测（阈值 0.5）\n",
    "        preds = (outputs > 0.5).float()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')  # 二分类情况\n",
    "\n",
    "print(\"Validation Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Validation F1 Score: {:.4f}\".format(f1))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 合取规则 ====\n",
      "conj0 = ¬P2\n",
      "conj1 = ¬P2 ∧ P5\n",
      "conj2 = P2 ∧ ¬P5\n",
      "conj6 = ¬P2\n",
      "conj7 = P2 ∧ ¬P5\n",
      "conj8 = P2\n",
      "conj9 = ¬P2 ∧ P5\n",
      "\n",
      "==== 最终分类规则 ====\n",
      "Class 0: conj1 ∨ ¬conj2 ∨ ¬conj3 ∨ conj6 ∨ ¬conj7 ∨ ¬conj8 ∨ conj9\n"
     ]
    }
   ],
   "source": [
    "# 训练完成后调用\n",
    "rules = model.dnf.get_rules(threshold=0.5)  # 可调整阈值\n",
    "\n",
    "# selected_features\n",
    "print(\"==== 合取规则 ====\")\n",
    "for rule in rules[\"conjuncts\"]:\n",
    "    if \"∅\" not in rule:  # 过滤空规则\n",
    "        print(rule)\n",
    "\n",
    "print(\"\\n==== 最终分类规则 ====\")\n",
    "for class_idx, rule in rules[\"disjuncts\"].items():\n",
    "    print(f\"Class {class_idx}: {rule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_rule(rule: str, feature_map: dict) -> str:\n",
    "    \"\"\"将符号规则转换为自然语言\"\"\"\n",
    "    explanation = []\n",
    "    for term in rule.split():\n",
    "        if \"P\" in term:\n",
    "            # 提取特征和符号\n",
    "            sign = \"非\" if \"¬\" in term else \"\"\n",
    "            p_num = term.split(\"P\")[-1]\n",
    "            explanation.append(f\"{sign}{feature_map[int(p_num)]}\")\n",
    "        elif \"conj\" in term:\n",
    "            explanation.append(f\"({term})\")\n",
    "    return \" 或 \".join(explanation).replace(\"∧\", \" 且 \").replace(\"∨\", \" 或 \")\n",
    "feature_map = {\n",
    "    1: \"信息充分性\", 2: \"信息准确性\", \n",
    "    3: \"内容完整性\", 4: \"意图正当性\",\n",
    "    5: \"发布者信誉\", 6: \"情感中立性\",\n",
    "    7: \"无诱导行为\", 8: \"信息一致性\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 基础特征组合规则\n",
      "- 当 非信息准确性 时触发该规则\n",
      "- 当 非信息准确性 或 发布者信誉 时触发该规则\n",
      "- 当 信息准确性 或 非发布者信誉 时触发该规则\n",
      "- 当 非信息准确性 时触发该规则\n",
      "- 当 信息准确性 或 非发布者信誉 时触发该规则\n",
      "- 当 信息准确性 时触发该规则\n",
      "- 当 非信息准确性 或 发布者信誉 时触发该规则\n",
      "\n",
      "## 最终决策逻辑\n",
      "### 虚假信息判定条件\n",
      "满足以下任一条件即判定为虚假信息：\n",
      "  - (conj1)\n",
      "  - (¬conj2)\n",
      "  - (¬conj3)\n",
      "  - (conj6)\n",
      "  - (¬conj7)\n",
      "  - (¬conj8)\n",
      "  - (conj9)\n"
     ]
    }
   ],
   "source": [
    "def generate_semantic_report(rules, feature_map):\n",
    "    \"\"\"生成完整语义报告\"\"\"\n",
    "    report = []\n",
    "    # 合取规则解释\n",
    "    report.append(\"## 基础特征组合规则\")\n",
    "    for conj in rules[\"conjuncts\"]:\n",
    "        if \"∅\" not in conj:\n",
    "            _, expr = conj.split(\"=\")\n",
    "            report.append(f\"- 当 {explain_rule(expr.strip(), feature_map)} 时触发该规则\")\n",
    "    \n",
    "    # 最终决策规则解释\n",
    "    report.append(\"\\n## 最终决策逻辑\")\n",
    "    for cls, rule in rules[\"disjuncts\"].items():\n",
    "        if \"∅\" not in rule:\n",
    "            cls_name = \"虚假信息\" if cls == 0 else \"真实信息\"\n",
    "            report.append(f\"### {cls_name}判定条件\")\n",
    "            report.append(f\"满足以下任一条件即判定为{cls_name}：\")\n",
    "            for term in rule.split(\"∨\"):\n",
    "                report.append(f\"  - {explain_rule(term.strip(), feature_map)}\")\n",
    "    return \"\\n\".join(report)\n",
    "print(generate_semantic_report(rules, feature_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机样本输入 shape: torch.Size([1, 8])\n",
      "随机样本标签: tensor([[0.]])\n",
      "随机样本模型预测输出: 0.08580563962459564\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # 从当前batch随机挑选一个样本\n",
    "        random_idx = random.randint(0, inputs.size(0) - 1)\n",
    "        single_input = inputs[random_idx : random_idx + 1]  # 保持维度一致\n",
    "        single_label = labels[random_idx : random_idx + 1]\n",
    "        print(\"随机样本输入 shape:\", single_input.shape)\n",
    "        print(\"随机样本标签:\", single_label)\n",
    "        break  # 只取第一个batch中的随机样\n",
    "    predicted = model(single_input)\n",
    "    print(\"随机样本模型预测输出:\", predicted.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reporter import generate_report\n",
    "generate_report(model, val_loader, feature_map, \"./reports/weibo_dnf_report.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
